# Unleashing Text-to-Image Diffusion Prior for Zero-Shot Image Captioning [ECCV24]

## SynthImgCap Dataset

The SynthImgCap dataset is a pioneering resource for advancing zero-shot image captioning research, which provides an alternative that reduces dependence on costly human-annotated paired data for Vision-and-Language tasks. The SynthImgCap dataset employs Stable Diffusion to synthesize an image for each sentence in the training corpus, resulting in 542,401 and 144,541 synthetic image-text pairs for MSCOCO-SD and Flickr30k-SD datasets, respectively. The dataset is released at [Google Drive](https://drive.google.com/drive/folders/19ASNWUL9-WaCj2RJTQBP2vxh25VlPWS3?usp=sharing) and [BaiduYun](https://pan.baidu.com/s/1ntmgvPO9hPBOLS0ecJtfWA) (extract code: `vpkx`).





## Training & Inference Code
Source code will be released soon ...
